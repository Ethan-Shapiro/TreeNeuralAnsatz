{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repository\\TreeNeuralAnsatz\\env\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 feature importances: [3.97998124e-06]\n"
     ]
    }
   ],
   "source": [
    "# Fetch the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Filter out the classes for binary classification of digits 0 and 1\n",
    "mask = (mnist.target == '0') | (mnist.target == '1')\n",
    "X_mnist, y_mnist = mnist.data[mask], mnist.target[mask].astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mnist, y_mnist, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get the prediction probabilities for class 1\n",
    "y_pred_proba = xgb_clf.predict_proba(X_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 feature importances: [3.97998124e-06]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the gradients for binary logistic loss\n",
    "gradients = y_pred_proba - y_train\n",
    "\n",
    "# Ensure gradients is a NumPy array\n",
    "gradients_np = np.array(gradients)\n",
    "\n",
    "# Calculate the outer products of the gradients\n",
    "outer_products = np.array([np.outer(g, g)\n",
    "                          for g in gradients_np[:, np.newaxis]])\n",
    "\n",
    "# Average the outer products to approximate the NFM\n",
    "nfm_approx = np.mean(outer_products, axis=0)\n",
    "\n",
    "# Feature importance based on the diagonal of the NFM matrix\n",
    "feature_importance = np.diag(nfm_approx)\n",
    "\n",
    "# Output the feature importances\n",
    "# Show the first 10 for brevity\n",
    "print(\"First 10 feature importances:\", feature_importance[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 feature importances: [3.97998109e-06]\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get the raw output scores (logits) for the train set\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "logits = xgb_clf.get_booster().predict(dtrain, output_margin=True)\n",
    "\n",
    "# Compute the gradient for binary logistic loss for each feature\n",
    "# Note: The gradients are computed w.r.t the logits, not the probabilities\n",
    "# For logistic loss: gradient = actual label - predicted probability\n",
    "# Convert logits to probabilities\n",
    "predicted_probability = 1.0 / (1.0 + np.exp(-logits))\n",
    "gradients = y_train - predicted_probability\n",
    "\n",
    "# Calculate the outer products of the gradients\n",
    "outer_products = np.array([np.outer(g, g) for g in gradients])\n",
    "\n",
    "# Average the outer products to approximate the NFM\n",
    "nfm_approx = np.mean(outer_products, axis=0)\n",
    "\n",
    "# Feature importance based on the diagonal of the NFM matrix\n",
    "feature_importance = np.diag(nfm_approx)\n",
    "\n",
    "# Output the feature importances\n",
    "# Show the first 10 for brevity\n",
    "print(\"First 10 feature importances:\", feature_importance[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 feature importances: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Function to perturb feature and get model's output change\n",
    "def perturb_and_predict(model, data, feature_index, epsilon=10):\n",
    "    # Perturb the feature\n",
    "    original_feature_values = data[:, feature_index].copy()\n",
    "    data[:, feature_index] += epsilon\n",
    "    predictions_plus_epsilon = model.predict_proba(data)[:, 1]\n",
    "\n",
    "    # Reset the feature\n",
    "    data[:, feature_index] = original_feature_values\n",
    "\n",
    "    # Calculate the change in predictions\n",
    "    dp = predictions_plus_epsilon - model.predict_proba(data)[:, 1]\n",
    "    return dp / epsilon\n",
    "\n",
    "\n",
    "# Initialize an array to store the summed outer products\n",
    "summed_outer_products = np.zeros((X_train.shape[1], X_train.shape[1]))\n",
    "\n",
    "# Approximate the gradients and update the summed outer products\n",
    "for feature_index in range(X_train.shape[1]):\n",
    "    # Get the approximate gradient for one feature across all samples\n",
    "    approx_gradients = perturb_and_predict(xgb_clf, X_train, feature_index)\n",
    "\n",
    "    # Update the summed outer products matrix with the outer product of the gradients\n",
    "    for grad in approx_gradients:\n",
    "        # Only need diagonal for feature importance\n",
    "        summed_outer_products[feature_index, feature_index] += grad**2\n",
    "\n",
    "# Feature importance is the sum of squares of the gradients along the diagonal\n",
    "feature_importance = np.diag(summed_outer_products)\n",
    "\n",
    "# Output the feature importances\n",
    "# Show the first 10 for brevity\n",
    "print(\"First 10 feature importances:\", feature_importance[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(feature_importance == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  Feature 0: Original Prediction = 0.9999626874923706, Changed Prediction = 0.9999626874923706\n",
      "  Feature 1: Original Prediction = 0.9999626874923706, Changed Prediction = 0.9999626874923706\n",
      "  Feature 2: Original Prediction = 0.9999626874923706, Changed Prediction = 0.9999626874923706\n",
      "  Feature 3: Original Prediction = 0.9999626874923706, Changed Prediction = 0.9999626874923706\n",
      "  Feature 4: Original Prediction = 0.9999626874923706, Changed Prediction = 0.9999626874923706\n",
      "Sample 1:\n",
      "  Feature 0: Original Prediction = 0.9999810457229614, Changed Prediction = 0.9999810457229614\n",
      "  Feature 1: Original Prediction = 0.9999810457229614, Changed Prediction = 0.9999810457229614\n",
      "  Feature 2: Original Prediction = 0.9999810457229614, Changed Prediction = 0.9999810457229614\n",
      "  Feature 3: Original Prediction = 0.9999810457229614, Changed Prediction = 0.9999810457229614\n",
      "  Feature 4: Original Prediction = 0.9999810457229614, Changed Prediction = 0.9999810457229614\n",
      "Sample 2:\n",
      "  Feature 0: Original Prediction = 3.621404175646603e-05, Changed Prediction = 3.621404175646603e-05\n",
      "  Feature 1: Original Prediction = 3.621404175646603e-05, Changed Prediction = 3.621404175646603e-05\n",
      "  Feature 2: Original Prediction = 3.621404175646603e-05, Changed Prediction = 3.621404175646603e-05\n",
      "  Feature 3: Original Prediction = 3.621404175646603e-05, Changed Prediction = 3.621404175646603e-05\n",
      "  Feature 4: Original Prediction = 3.621404175646603e-05, Changed Prediction = 3.621404175646603e-05\n"
     ]
    }
   ],
   "source": [
    "# Choose a small number of samples for inspection\n",
    "sample_indices = [0, 1, 2]  # Inspect the first three samples\n",
    "feature_indices = [0, 1, 2, 3, 4]  # Inspect the first five features\n",
    "\n",
    "for sample_index in sample_indices:\n",
    "    print(f\"Sample {sample_index}:\")\n",
    "    for feature_index in feature_indices:\n",
    "        original_value = X_train[sample_index, feature_index]\n",
    "        X_train[sample_index, feature_index] = original_value + \\\n",
    "            100  # Increase feature by 1\n",
    "        pred_changed = xgb_clf.predict_proba(\n",
    "            X_train[sample_index:sample_index+1])[0, 1]\n",
    "        # Reset the feature\n",
    "        X_train[sample_index, feature_index] = original_value\n",
    "        pred_original = xgb_clf.predict_proba(\n",
    "            X_train[sample_index:sample_index+1])[0, 1]\n",
    "\n",
    "        print(\n",
    "            f\"  Feature {feature_index}: Original Prediction = {pred_original}, Changed Prediction = {pred_changed}\")\n",
    "\n",
    "# Reset the dataset after inspection\n",
    "# Assuming original data is standardized\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
